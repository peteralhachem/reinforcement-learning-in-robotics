{"cells":[{"cell_type":"markdown","metadata":{"id":"Siz3Hiq1JUZ1"},"source":["Install and load all dependencies (first time only) \\\n","NOTE: you may need to restart the runtime afterwards (CTRL+M .)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AW6XT0jSJI8e"},"outputs":[],"source":["!apt-get install -y \\\n","    libgl1-mesa-dev \\\n","    libgl1-mesa-glx \\\n","    libglew-dev \\\n","    libosmesa6-dev \\\n","    software-properties-common\n","\n","!apt-get install -y patchelf\n","\n","!pip install gym\n","!pip install free-mujoco-py\n","!pip install stable_baselines3[extra]"]},{"cell_type":"markdown","metadata":{"id":"gwIRXGd5K3xJ"},"source":["Set up the custom Hopper environment and provided util functions\n","\n","\n","\n","1.   Upload the corresponding zip hopper to the current session's file storage:\n","\n","\n","*   `our_hopper.zip`\n","\n","\n","2.   Un-zip it by running cell below\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9WsofDVLaCC"},"outputs":[],"source":["#!unzip custom_hopper.zip\n","!unzip our_hopper.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTYmUufrJTNl"},"outputs":[],"source":["import gym\n","from env.customHopper_dr import *\n","import numpy as np"]},{"cell_type":"markdown","source":["Instantiate the environments needed for the agent: the parameters printed by the udr environment are based on the 'uniform distribution' with a value range for different components of our environment: **[-1,1]** ."],"metadata":{"id":"RC7r8o2y4TF4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcCfCGg-Jyc3"},"outputs":[],"source":["source_env = gym.make('CustomHopper-source-v0') # [2.53429174 3.92699082 2.71433605 5.0893801 ]\n","target_env = gym.make('CustomHopper-target-v0') # [3.53429174 3.92699082 2.71433605 5.0893801 ]\n","udr_env = gym.make('CustomHopper-domain_randomization-v0')\n","\n","\n","print('State space:', source_env.observation_space)   # state-space\n","print('Action space:', source_env.action_space)       # action-space\n","print('Dynamics parameters:', source_env.get_parameters())  # masses of each link of the Hopper\n","print('Dynamics parameters target:', target_env.get_parameters())  # masses of each link of the Hopper\n","print('Dynamics parameters DR:', udr_env.get_parameters()) # masses of each link domain randomization"]},{"cell_type":"markdown","source":["\n","We take the images as an input and we process them before sending them to the feature extractor\n","1.  stack the images into one dimensional array.\n","2.  convert them into tensor for GPU (cuda).\n","3.  Put them on the appropriate device.\n","\n"],"metadata":{"id":"J_A67iGW7Cur"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Adv0N5acwftf"},"outputs":[],"source":["import numpy as np\n","import torch\n","from gym.wrappers.pixel_observation import PixelObservationWrapper\n","\n","\n","\n","def stack_pixel_observations(pixel_obs: list):\n","    \"\"\"\n","    Stack only as many observations there are in the list, since want to be able to handle both 'reset'\n","    situation but also when 'done'==True\n","    \"\"\"\n","    stacked = np.stack(pixel_obs[:])\n","    return stacked\n","\n","\n","\n","def stacked_observations_to_tensor(stacked_observations):\n","    \"\"\"\n","    take in stacked observations of len ranging from 1 to 4, turn into tensor\n","    \"\"\"\n","    n = len(stacked_observations) #number of observations since stacked on axis=0\n","    tensor = torch.from_numpy(stacked_observations).view(n, 3, 224, 224)\n","    return tensor\n","\n","\n","def preprocess_n_observations(pixel_observations):\n","    \"\"\"\n","    take in observations from 'reset' or 'step', preprocess them and return tensor to be\n","    used as input in alexnet\n","    \"\"\"\n","    stacked = stack_pixel_observations(pixel_observations)\n","    tensor = stacked_observations_to_tensor(stacked)\n","    tensor = tensor.type(torch.float32) #change type\n","    tensor = tensor.to(torch.device(device))  #move to cuda\n","    return tensor\n"]},{"cell_type":"markdown","source":["Importing `AlexNet`,  will be used as a feature extractor."],"metadata":{"id":"1lcFMoOa6YQ3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LM18LasJ_HsJ"},"outputs":[],"source":["# importing AlexNet and extract features\n","# Create CNN model using AlexNet\n","import torchvision.models as models\n","\n","# Instantiate an AlexNet model\n","alexnet = models.alexnet(weights=\"DEFAULT\")\n","\n","# Move the model to the GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","alexnet = alexnet.to(device)"]},{"cell_type":"markdown","source":["Implementation of Feature Extractor Environment that takes preprossessed images as an input and use them as an observation space instead of angle and velocity and dimensions of the Hopper"],"metadata":{"id":"Zbuwlhqb6kj7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBQPUkAPHQqS"},"outputs":[],"source":["from gym import spaces\n","\n","class FeatureExtractorEnv(gym.Wrapper):\n","    def __init__(self, env, feature_extractor):\n","        super().__init__(env)\n","        self.feature_extractor = feature_extractor\n","        # attempt to fix shape difficulties\n","        self.observation_space = spaces.Box(low=0, high=1,\n","                                        shape=(1000,), dtype=np.uint8)\n","        self.feature_list = []\n","\n","    def reset(self):\n","        observations = []\n","        normal_obs = self.env.reset() # resetting the environment taken as input from the gym.Wrapper\n","        obs = source_env.render(mode=\"rgb_array\", width=224, height=224) / 255 #sizing the images and performing a normalization so all the values bound between [0,1]\n","        observations.append(obs)\n","        observations_features = self.feature_extractor(preprocess_n_observations(observations))\n","        observations_numpy = observations_features.data.cpu().numpy()\n","\n","        return observations_numpy\n","\n","    def step(self, action):\n","        done = False\n","        iter_ = 0\n","        observations = []\n","        tot_reward = 0\n","        #The iteration is the value that represent sequence of images before moving on with the step of a new observation state.\n","        while not done and iter_ < 4:\n","            obs, reward, done, info = self.env.step(action)\n","            obs = source_env.render(mode=\"rgb_array\", width=224, height=224) / 255\n","            observations.append(obs)\n","            iter_ += 1\n","            tot_reward += reward\n","        observations_features = self.feature_extractor(preprocess_n_observations(observations))\n","        observations_numpy = observations_features.data.cpu().numpy()\n","        observations_numpy = np.sum(observations_numpy, axis=0)\n","        self.feature_list.append(observations_numpy)\n","        return observations_numpy, tot_reward, done, info\n","\n","\n"]},{"cell_type":"markdown","source":["Create Instances of the Feature Extractor environment considering the absence\n","and presence of ***Domain Randomization***."],"metadata":{"id":"8NZ1FahOPviW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dt8uwg_a7Aj-"},"outputs":[],"source":["import tqdm\n","from stable_baselines3 import SAC\n","\n","# source environment based on the feature extractor without domain randomization\n","new_env = FeatureExtractorEnv(source_env, alexnet)\n","\n","# UDR environment  based on the feature extractor with domain randomization\n","new_udr_env = FeatureExtractorEnv(udr_env, alexnet)\n"]},{"cell_type":"markdown","source":["***`EvalCallback`***: Evaluate periodically the performance of an agent. It will save the best model if `best_model_save_path` folder is specified and save the evaluations results in a numpy archive `(evaluations.npz)` if `log_path` folder is specified."],"metadata":{"id":"Q0Hd8Ov1QOQ4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zdihRHLaPyFg"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","# Use deterministic actions for evaluation\n","eval_callback = EvalCallback(new_env, best_model_save_path=\"./logs/\",\n","                             log_path=\"./logs/\", eval_freq=3_000,\n","                             deterministic=True, render=False)"]},{"cell_type":"markdown","source":["***Soft Actor Critic (SAC)***: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor."],"metadata":{"id":"_ZRa7GA8QRlU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GzYahtKIWsKh"},"outputs":[],"source":["# Instantiate the RL algorithm for source\n","model = SAC(\"MlpPolicy\", new_udr_env,verbose=0, device=\"auto\", buffer_size = 10_000)\n","\n","# Train the RL agent on the environment\n","model.learn(total_timesteps=50_000, callback=eval_callback, log_interval=4, progress_bar=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC21yTE6QrGH"},"outputs":[],"source":["# display improvement during training\n","data_dict = dict(np.load('/content/logs/evaluations.npz'))\n","timesteps = data_dict['timesteps']\n","rewards = data_dict['results']\n","ep_length = data_dict['ep_lengths']"]},{"cell_type":"markdown","source":["***`evaluate_policy`***: Runs policy for `n_eval_episodes` episodes and returns average reward. It technically evaluates the trained policy on the environment chosen."],"metadata":{"id":"9GXCo3uVQcY3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVpcYJjHQ6-f"},"outputs":[],"source":["from stable_baselines3.common.evaluation import evaluate_policy\n","\n","# source - source reward calculation\n","mean_reward, std_reward = evaluate_policy(source, new_env, n_eval_episodes=50)\n","print(f\"mean_reward new_source-new_source:{mean_reward:.2f} +/- {std_reward:.2f}\")\n","\n","\n","# source - target reward calculation\n","mean_reward, std_reward = evaluate_policy(source, new_udr_env, n_eval_episodes=50)\n","print(f\"mean_reward source-target:{mean_reward:.2f} +/- {std_reward:.2f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTlPhrnVQ5M-"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.style as style\n","import matplotlib as mpl\n","\n","# back to default settings\n","mpl.rcParams.update(mpl.rcParamsDefault)\n","\n","# use a theme\n","style.use('seaborn-notebook')\n","\n","x = timesteps/1000\n","means = np.mean(rewards, axis=1)\n","stds = np.std(rewards, axis=1)\n","ep_length_mean = np.mean(ep_length, axis=1)\n","\n","fig = plt.figure()\n","ax1 = fig.add_subplot(111)\n","ax2 = ax1.twinx()\n","\n","lns1 = ax1.plot(x, means, label='Mean reward')\n","lns2 = ax1.fill_between(x, means - stds, means + stds, alpha=0.3, label='Deviation')\n","lns3 = ax2.plot(x, ep_length_mean, 'k-', alpha=0.5, label='Mean episode length')\n","\n","lns = lns1+[lns2]+lns3\n","labs = [line.get_label() for line in lns]\n","ax1.legend(lns, labs, loc='lower right')\n","\n","ax1.grid()\n","plt.title('Training rewards for uniform weight perturbations', fontsize=15) #, fontsize=20, weight = 'bold'\n","ax1.set_xlabel('Timesteps (thousands)', weight='bold') #, fontsize=15, weight='bold'\n","ax1.set_ylabel('Reward', weight='bold') #, fontsize=15, weight='bold'\n","ax2.set_ylabel('Episode length', weight='bold') #, fontsize=15, weight='bold'\n","\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}